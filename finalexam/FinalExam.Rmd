---
title: Final Exam
output: html_document
---

# Final Exam

* Date: 2020/1/6
* Class: Multivariate
* Student: R08631020 Lin, Hong-Ye

<div style="height:50vh"></div>
## Q1 Grade data Analysis
``` {R}
set.seed(1)
data = read.csv("./GRADES.dat", header=T, sep="")
head(data)
```

### (a) Clustering

Using K-mean to cluster
``` {R}
cl = kmeans(data, 3)
cl
```

And show the first two axis of PCA to evaluate the cluster.
``` {R}
pc_data = princomp(data ,cor=TRUE)
new_data = t(t(pc_data$loadings[, 1:2]) %*% t(as.matrix(data)))
plot(new_data,col=cl$cluster)
points(cl$centers, col=1:2, pch=8, cex=2)
```

And there mean performance in each cluster (Black, Red, Green)
``` {R}
cl$centers
```

### (b) Factor Analysis

Perform FA

``` {R}
library(psych)
set.seed(1)
data = read.csv("./GRADES.dat", header=T, sep="")
head(data)
data_cor = head(data)
fa_data = principal(data_cor, nfactors=4, rotate='varimax')
print(fa_data$loading, cutoff=.5)
```

You can see the result show that Lab score has high correlated to HW, exam1 and exam2.

Popquiz is related to exam1 and finalexam. Also in other component, it related to exam2.

And in the scree plot, choosing three factors is enough.

``` {R}
a = colSums(fa_data$loadings ^2)
a$sdev = colSums(fa_data$loadings ^2)
screeplot(a)
abline(h=1, col="red")
```

<div style="height:50vh"></div>
## Q2 Grade data Analysis
``` {R}
set.seed(1)
data = read.csv("./GRADES.dat", header=T, sep="")
head(data)
d = dist(data)
```

### (a)(b) Clustering and find the minial

Using hclust to cluster to two groups by each method.
``` {R}
method = c("ward.D", "ward.D2", "single", "complete", "average", "median", "mcquitty", "centroid")

for (i in 1:length(method)) {
    dc = hclust(d, method=method[i])
    memb = cutree(dc, k = 2)
    d1 = colMeans(data[memb == 1,])
    d2 = colMeans(data[memb == 2,])
    dd = t(data.frame(d1, d2))
    dd = data.frame(dd, len=c(sum(memb == 1), sum(memb == 2)))
    print(method[i])
    print(dd)
}
```

The result show that `ward.D` and `mcquitty` make the minial students to fail by the distance of those 6 dimensions. It only fail 13 students.

Or you use `single` to cut, only 1 students will fail, however his/her lab and homework is somewhat better than another groups.

<div style="height:50vh"></div>
## Q3 paper strength analysis

``` {R}
set.seed(1)
data = read.csv("./strength.dat", header=T, sep="\t")
head(data)
X = data[, c("X1", "X2", "X3", "X4")]
Y = data[, c("Y1", "Y2", "Y3", "Y4")]
```

### (a) Canonical analysis
``` {R}
library(CCA)
result <-cc(X, Y)
barplot(result$cor)
# plt.cc(result)
"Correlation"
result$cor
"X loadings"
result$xcoef
"Y loadings"
result$ycoef
```

We can find first two axis are good enough(have higher correlations).

### (b) Explain first variates of cca

Try to rotate the axis by first loadings.
``` {R}
nx = as.matrix(X) %*% result$xcoef[, 1]
ny = as.matrix(Y) %*% result$ycoef[, 1]
n = data.frame(nx, ny)
```

Then, simply feed in to linear regression.

``` {R}
fit = lm(ny ~ nx, n)
summary(fit)
plot(nx, ny)
abline(fit)
```

Where it's $R^2$ is 0.8389, both p values of the slope and intercept are close to 0, so I think is a good measurment.

or you can evalute by testing
``` {R}
n = nrow(data)
dimx = ncol(X)
dimy = ncol(Y)
# Bartlett's test
test =  -( n - 0.5 * (dimx + dimy + 3) ) * log(1 - result$cor[1])
"P value"
test_p = pchisq(test, df=dimx + dimy - 1 + 1, lower.tail=FALSE)
test_p
```

Which get the same result as above that p value is close to 0.


<div style="height:50vh"></div>
## Q4 Food expendition Analysis

### (a) True

Rewrite the data. I separate the type and numbers of childrens `X` to `MA`, `CA`, `EM` and `num`.
``` {R}
set.seed(1)
data = read.csv("./food.dat", header=T, sep=" ")

# Type: MA2 EM2 CA2 MA3 EM3 CA3 MA4 EM4 CA4 MA5 EM5 CA5
# I don't know how to split the sting in R so manually do it...
tp = factor(c("MA", "EM", "CA", "MA", "EM", "CA", "MA", "EM", "CA", "MA", "EM", "CA"))
data = cbind(data, num=c(2 ,2 ,2 ,3 ,3 ,3 ,4 ,4 ,4 ,5 ,5 ,5), MA=as.numeric(tp == "MA"), EM=as.numeric(tp == "EM"), CA=as.numeric(tp == "CA"))
data
```

### (b) Corresponse analysis
``` {R}
# X = data[, c("num", "MA", "EM", "CA")]
X = data[, c("num", "MA", "EM", "CA")]
Y = data[,c("Bread", "Vegetables", "Fruit", "Meat", "Poultry", "Milk", "Wine")]

library("factoextra")
library("FactoMineR")
library("gplots")
fit <-CA(X, Y, graph = FALSE)
print(fit)
fviz_ca_biplot(fit)
```

All components are high related in the chart.

And the numbers in family all related to the porpotional of expension


<div style="height:50vh"></div>
## Q5 Explain

### (a) MDS differ from Cluster and Factor Analysis

Multidimension scaling try to restore the coordinations from distance data. One of the advantage is that it can reduce the dimensions of data(Similar to FA), the other advantage is restoring corrdinate with the rank of distance not the real value of it's distance, this is a little like clustering.

### (b) Clustering vs Factor Analysis

Cluster is trying to group the data by it's distance usually.

Factor analysis is to find the latent feature of the data which show the relationship of them.
